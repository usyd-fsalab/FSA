<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>FSA Labs - Machine Learning Research</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<div class="hero-image" style='background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url("images/bg/opera_t10.png");'>
				<header id="header">
					<div class="inner">

						<!-- Logo -->
						<a href="index.html" class="logo">
							<span class="title">
								<svg width="160" height="160">
									<image xlink:href="assets/logo/logo.svg" width="160" height="160"/>
								</svg>
							</span>
							<div class="hero-text"><span class="title">Future Systems Architecture<br /> Laboratory</span></div>

						</a>

						<!-- Nav -->
						<nav>
							<ul>
								<li><a href="#menu">Menu</a></li>
							</ul>
						</nav>
					</div>
				</header>
			</div>

			<!-- Menu -->
			<nav id="menu">
				<h2>Menu</h2>
				<ul>
					<li><a href="index.html">Home</a></li>
					<li><a href="news.html">News</a></li>
					<li><a href="blog.html">Blog</a></li>
					<li><a href="seminars.html">Seminars</a></li>
					<li><a href="topic_quantum.html">Quantum Computing Research</a></li>
					<li><a href="topic_ml.html">Machine Learning Research</a></li>
					<li><a href="topic_vr.html">VR/AR Hardware Research</a></li>
					<li><a href="topic_hpc.html">High Performance Computing Research</a></li>
				</ul>
			</nav>

			<!-- Main -->
			<div id="main">
				<div class="inner">
					<br/>
					<h1>Machine Learning System (MLSys) Research</h1>
					<span class="image main"><img src="images/ml/topic-image.jpeg" alt="" /></span>
					
						<p>Machine learning has been widely used in a wide spectrum of modern application domains such as recommendation, computer vision, natural language processing, etc. The performance of inference is crucial to deploy pretrained models into production. With the development of new machine learning models
						and hardware architectures, a set of new challenges emerge from efficient executing of inference jobs for both large models and small models. On the one hand, ultra-scale models pretraining becomes increasingly popular in recent years, while how to deploy these large models
						efficiently onto hardware platforms with as minimum resource usage is still not well studied. On the other hand, small-scale models face the performance issues of non-computation overhead, which becomes an increasingly important factor for end-to-end performance with the boost of
						computing power from both general-purpose (e.g., GPUs) and customised ML accelerators. At FSA, we are particularly interested in tackling the essential performance problems above for both extreme large-scale and small-scale models on a diverse range of hardware platforms. Our team has a unique capability of collaborating with
					    top industry research labs to use real-world enterprise scenarios to constrain and refine our solutions that can scale to millions of users. </p>
					<p>Along with our top international academic and industry lab collaborators (Google Brain, Microsoft and Alibaba), FSA aims to explore principles and key technologies of multi-scale multi-dimensional machine learning inference system optimisation through cross-stack co-design (compiler, runtime and hardware accelerators).
						The scope of our MLSys resh includes but not limited to ML compiler design and optimisations, software-hardware co-design, runtime opa techniques, and customized acceleration for novel deep learning models. </p>

					<section>
						<hr/>
						<br/>
						<h2>Publications</h2>
<div class="row">
	<div class="col-12 col-12-medium">
		<ul>
		<li>AStitch: enabling a new multi-dimensional optimisation space for memory-intensive ML training and inference on modern SIMT architectures. <b>ASPLOS 2022.</b></li>
		<li>Randomness In Neural Network Training: Characterising The Impact of Tooling, <b>MLSys 2022</b>, with Google Brain.</li>
		<li> COMET: A Novel Memory-Efficient Deep Learning Training Framework by Using ErrorBounded Lossy Compression, <b>VLDB'22</b></li>
		<li>MalFox:Camouflaged Adversarial Malware Example Generation Based on Conv-GANs Against Black-Box Detectors, <b>IEEE Transactions on Computers, 2022</b></li>
		<li>Enabling Highly Efficient Capsule Networks Processing Through Software-Hardware Co-Design. <b>IEEE Transactions on Computers, 2021</b></li>
		<li>ClickTrain: efficient and accurate end-to-end deep learning training via fine-grained architecture-preserving pruning. <b>ICS 2021.</b></li>
		<li>η-LSTM: Co-Designing Highly-Efficient Large LSTM Training via Exploiting Memory-Saving and Architectural Design Opportunities. <b>ISCA 2021</b></li>
		<li>Shift-BNN: Highly-Efficient Probabilistic Bayesian Neural Network Training via Memory-Friendly Pattern Retrieving. <b>MICRO 2021</b></li>
		<li>MAPA: Multi-Accelerator Pattern Allocation Policy for Multi-Tenant GPU Servers, <b>SC 2021.</b></li>
		<li>Toward efficient interactions between Python and native libraries, <b>ESEC/FSE’21.</b></li>
		<li>A novel memory-efficient deep learning training framework via error-bounded lossy compression, <b>PPoPP'21</b></li>
		<li> Enabling Highly Efficient Capsule Networks Processing Through A PIM-Based Architecture Design, <b>HPCA' 20.</b></li>
		<li>BSTC: A Novel Binarized Soft Tensor Core Design for Accelerating Bit-Based Approximated Neural Nets, <b>SC 2019.</b></li>
		<li> LP-BNN: Ultra low latency BNN inference with layer parallelism, <b>ASAP 2019.</b></li>
		<li>SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks., <b>PPoPP 2018</b></li>
		<li>NUMA-Caffe: NUMA-Aware Deep Learning Neural Networks, <b>TACO 2018</b></li>
		<li>MIC-SVM: Designing A Highly Efficient Support Vector Machine For Advanced Modern Multi-Core and Many-Core Architectures, <b>IPDPS 2014</b></li>
</ul>
</div>
</div>
</section>

				</div>
			</div>
			<!-- Footer -->
			<footer id="footer">
				<section>
					<h2>Our Supporters and Collaborators</h2>
					<ul class="icons">
						<li>
							<a href="https://nvidia.com" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/nvidia.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://microsoft.com" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/microsoft.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://facebook.com" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/facebook.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://google.com" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/google.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://intel.com" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/intel.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://alibabagroup.com" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/alibaba.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://sydney.edu.au" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/usyd.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://www.washington.edu/" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/uw.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://www.pnnl.gov/" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/pnnl.png" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://www.energy.gov/" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/doe.svg" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://science.osti.gov/" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/doesc.png" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
						<li>
							<a href="https://www.arc.gov.au/" class="icon brands">
								<span class="symbol">
									<svg width="120" height="120">
										<image xlink:href="assets/logo/arc.png" width="120" height="120"/>
									</svg>
								</span>
							</a>
						</li>
					</ul>
					<!-- <ul class="copyright"> -->
						<!-- 	<li>&copy; Untitled. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li> -->
						<!-- </ul> -->
				</section>
			</footer>
		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
	</body>
</html>
